{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad4a405-6e25-4c6e-8c35-8b07e72abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor, MultiRegressionCustomMetric\n",
    "from pycbrf import ExchangeRates\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c9cbc8-5910-4df4-932b-a233bef66bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from navec import Navec\n",
    "import holidays\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6a9691-cf99-4366-972a-7fdd3d86f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b822f1-2bdc-43ca-ba6f-efc6db9f9014",
   "metadata": {},
   "source": [
    "## Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a5734-097c-474a-a11c-af23a5fab84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_dataset_train.csv')\n",
    "test_data = pd.read_csv('test_dataset_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b11538-3400-4880-a99a-2fe5c33a8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solution(pred, filename): \n",
    "    solution = pd.DataFrame(test_data['document_id'])\n",
    "    solution[['views', 'depth', 'full_reads_percent']] = pred\n",
    "    solution.to_csv(filename, index=False)\n",
    "\n",
    "def parse_title(title):\n",
    "    title = title.split('\\n')\n",
    "    if len(title) == 2:\n",
    "        title = title[0] + '. ' + title[1].strip()\n",
    "    else:\n",
    "        title = title[0]\n",
    "    return title\n",
    "\n",
    "def get_USD_rate(date):\n",
    "    if date in rates:\n",
    "        current_rate = rates[date]\n",
    "    else:\n",
    "        current_rate = rates[date] = ExchangeRates(date)['USD'].value\n",
    "    \n",
    "    prev = date-pd.Timedelta(days=1)\n",
    "    if prev in rates:\n",
    "        prev_rate = rates[prev]\n",
    "    else:\n",
    "        prev_rate = rates[prev] = ExchangeRates(prev)['USD'].value\n",
    "    return current_rate - prev_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28896b69-179f-482b-96bb-9486463518a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    data['clean_title'] = data['title'].apply(parse_title) # очистка названия статьи\n",
    "    data['num_words'] = data.title.apply(lambda x: len(x.split())) # количество слов в заголовке\n",
    "    data['num_authors'] = data.authors.apply(eval)\n",
    "    data['num_authors'] = data.num_authors.apply(len) #количество авторов\n",
    "    data['num_tags'] = data.tags.apply(eval)\n",
    "    data['num_tags'] = data.num_tags.apply(len) # количество тегов\n",
    "    data['USD_difference'] = pd.to_datetime(data.publish_date).dt.date.apply(get_USD_rate) # курс доллара на дату публикации\n",
    "    data['is_group_authors'] = data.num_authors > 1\n",
    "    data['is_group_authors'] = data.is_group_authors.astype(int) # один автор или группа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824318a3-ff48-4fba-8ffc-5ac62653140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([214, 948, 2424, 2438, 3603, 3756, 4183, 5086, 5337, 5634, 5923, 5951, 6198, 6359], inplace=True) #Статьи на китайском и английском, выбросы\n",
    "data.drop(data[data.category.isin(data.category.value_counts()[-3:].index)].index, inplace=True) # Убираем статьи категории \"дискуссионный клуб\" (8 шт, отсутствует в тесте)\n",
    "\n",
    "# ПРЕДОБРАБОТКА\n",
    "rates = {}\n",
    "extract_features(data)\n",
    "extract_features(test_data)\n",
    "\n",
    "# ДОБАВЛЕНИЕ ТЕКСТОВЫХ ТЭГОВ В ДАТАСЕТ\n",
    "tags_df = pd.read_csv('parsed_tags.csv')\n",
    "tags_df.tags_text = tags_df.tags_text.apply(lambda x: ' '.join(eval(x)))\n",
    "data = data.merge(tags_df, how='left', on='document_id')\n",
    "test_data = test_data.merge(tags_df, how='left', on='document_id')\n",
    "\n",
    "# УДАЛЕНИЕ ДУБЛИРУЮЩИХСЯ СТАТЕЙ\n",
    "nonunique_titles = data.clean_title.value_counts()\n",
    "nonunique_titles = nonunique_titles[nonunique_titles>1].index.to_list()\n",
    "multiple_titles = data[data.clean_title.isin(nonunique_titles)].index.to_list()\n",
    "data.drop(multiple_titles, inplace=True)\n",
    "data = data.reset_index()\n",
    "data.views = data.views.apply(np.log)\n",
    "\n",
    "targets = ['views', 'depth', 'full_reads_percent']\n",
    "y = data[targets]\n",
    "X = data.drop(targets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d70ee-0a29-4d07-90c2-77603a212f57",
   "metadata": {},
   "source": [
    "## Энкодеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f41cbf24-4bd4-4a27-99e9-18e908c0298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatetimeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, day_split=12):\n",
    "        self.feature_names = []\n",
    "        self.day_split = day_split\n",
    "        self.hol = holidays.Russia(years=[2017, 2018, 2019, 2020, 2021, 2022])\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = pd.to_datetime(X) + pd.Timedelta('03:00:00')\n",
    "        _X = pd.DataFrame()\n",
    "        # Создание колонок со временем суток публикации\n",
    "        _X['time_of_day'] = pd.cut(X.dt.strftime(\"%H\").astype(int), [i for i in range(0, 25, int(24/self.day_split))], labels=range(self.day_split), right=False)\n",
    "        # Создание колонок с годом, месяцем и днем публикации\n",
    "        #for col in ['year', 'month', 'day']:\n",
    "        #    _X[col] = X.dt.strftime(f\"%{col[0]}\").astype(int)\n",
    "        _X['day'] = X.dt.strftime('%d').astype(int)\n",
    "        _X['month'] = X.dt.strftime('%m-%y')\n",
    "        # Создание колонки, показывающей создана ли публикация в будни\n",
    "        _X['is_dayoff'] = X.dt.strftime(\"%w\").astype(int).isin([0,6]).astype(int) | X.dt.date.isin(self.hol)\n",
    "        _X['time_difference'] = (pd.to_datetime('2022-05-29')-X).dt.days\n",
    "        #_X.drop(['publish_date'], axis=1, inplace=True)\n",
    "        self.feature_names = np.array(_X.columns)\n",
    "        return _X\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1201dd7-5e1c-45b1-846d-79c0cd0142bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoderDP(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,  distance_threshold=0.4):\n",
    "        self.feature_names = ['clustering']\n",
    "        self.distance_threshold = distance_threshold\n",
    "        \n",
    "    def fit(self, title, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, title, y=None):         \n",
    "        embedder = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')\n",
    "\n",
    "        embeddings = embedder.encode(title)\n",
    "        print(embeddings)\n",
    "        embeddings = embeddings /  np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "        clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='average', distance_threshold=self.distance_threshold)\n",
    "        clustering_model.fit(embeddings)\n",
    "        res = pd.DataFrame(clustering_model.labels_, index=title.index)\n",
    "        return res\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70c09d9b-f70b-4497-83b1-f5215b21ec61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TitleEncoderRUBERT(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,  n_clusters=10):\n",
    "        self.feature_names = []\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def fit(self, title, y=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "        self.model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, title, y=None):         \n",
    "        X = pd.DataFrame({'title': title})\n",
    "        X['vectors'] = X['title'].apply(self.get_sentence_vec)\n",
    "        X = X.join(pd.DataFrame(X['vectors'].tolist(), index = X.index))\n",
    "        X.drop(['title', 'vectors'], axis=1, inplace=True)\n",
    "        km = KMeans(n_clusters=self.n_clusters, max_iter=10000, n_init=100, random_state=RANDOM_STATE).fit(X)\n",
    "        res = pd.DataFrame(km.labels_, index=X.index, columns=['clustering'])\n",
    "        self.feature_names = res.columns\n",
    "        return res\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names\n",
    "     \n",
    "    def get_sentence_vec(self, sentence):\n",
    "        t = self.tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**{k: v.to(self.model.device) for k, v in t.items()})\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "        return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f8d1ac-fdac-4649-8b9c-86b74f0e712e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TitleEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters=10):\n",
    "        self.feature_names = []\n",
    "        self.navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "        self.navec_voc = self.navec.vocab.words\n",
    "        self.missing_counter = []\n",
    "        self.abnormal = []\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def fit(self, title, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, title, y=None):\n",
    "        X = pd.DataFrame({'title': title})\n",
    "        X['vectors'] = X['title'].apply(self.get_sentence_vec)\n",
    "        X = X.join(pd.DataFrame(X['vectors'].tolist(), index = X.index))\n",
    "        X.drop(['title', 'vectors'], axis=1, inplace=True)\n",
    "        km = KMeans(n_clusters=self.n_clusters, max_iter=10000, n_init=100, random_state=RANDOM_STATE).fit(X)\n",
    "        res = pd.DataFrame(km.labels_, index=X.index, columns=['clustering'])\n",
    "        self.feature_names = res.columns\n",
    "        return res\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = word_tokenize(sentence, language='russian')\n",
    "        tokens = [i.lower() for i in tokens if i.isalpha()]\n",
    "        return tokens    \n",
    "    \n",
    "    def find_missing(self, missing):\n",
    "        snowball = SnowballStemmer(language='russian')\n",
    "        missing = [snowball.stem(i) for i in missing]\n",
    "        res = []\n",
    "        for w in missing:\n",
    "            if not w:\n",
    "                pass\n",
    "                #res.append(synonyms[w])\n",
    "            else:\n",
    "                r = re.compile(r'\\b' + w + r'\\w*\\b')\n",
    "                search = list(filter(r.match, self.navec_voc))\n",
    "                if search:\n",
    "                    for i in search:\n",
    "                        if i in self.navec:\n",
    "                            res.append(i)\n",
    "                            break\n",
    "                else:\n",
    "                    self.missing_counter.append(w)\n",
    "        return res\n",
    "    \n",
    "    def get_sentence_vec(self, title):\n",
    "        stop_words = stopwords.words('russian')\n",
    "        title = self.tokenize_sentence(title)\n",
    "        title = [i for i in title if i not in stop_words]\n",
    "        title_lst = [i for i in title if i in self.navec]\n",
    "        title_lst += self.find_missing([title[i] for i in range(len(title)) if title[i] not in title_lst])\n",
    "        #print(title_lst)\n",
    "        if len(title_lst) !=0:\n",
    "            vec = sum([self.navec.get(w) for w in title_lst])/(len(title_lst)+0.001) \n",
    "        else:\n",
    "            self.abnormal.append(title)\n",
    "            vec = np.zeros((300,))\n",
    "        return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32a187a1-bae6-4e36-8d1f-a9606fbde063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OHE_for_list(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, prefix=True, unique_values={}):\n",
    "        self.feature_names = []\n",
    "        self.prefix = prefix\n",
    "        self.unique_values = unique_values\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\"Распаковывает переданную в него серию, содержащую списки значений, \n",
    "        возвращает OHE-like матрицу размером n x m (n - кол-во уникальных значений, m - кол-во объектов)\"\"\"\n",
    "        X = pd.DataFrame(X) # если передана одна колонка в виде Series\n",
    "        result_df = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            if not self.unique_values.get(col):\n",
    "                col_unique = []\n",
    "                for lst in X[col].apply(eval):\n",
    "                    col_unique += lst\n",
    "                self.unique_values[col] = list(set(col_unique))\n",
    "                \n",
    "            _X = pd.DataFrame({'list': X[col]})\n",
    "            \n",
    "            if self.prefix:\n",
    "                _X = pd.DataFrame({f'{col}_{i}': _X.list.str.contains(i).astype(int) for i in self.unique_values[col]})\n",
    "            else:\n",
    "                _X = pd.DataFrame({i: _X.list.str.contains(i).astype(int) for i in self.unique_values[col]})\n",
    "            \n",
    "            result_df = pd.concat([result_df, _X.copy()], axis=1)\n",
    "        \n",
    "        self.feature_names = np.array(result_df.columns)\n",
    "        return result_df\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17060751-f1bf-4c3f-b1f6-5c540dcca90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Custom_R2_Metric(MultiRegressionCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 3\n",
    "        assert len(target[0]) == len(approxes[0])\n",
    "        score_views = r2_score(target[0], approxes[0])\n",
    "        score_depth = r2_score(target[1], approxes[1])\n",
    "        score_frp = r2_score(target[2], approxes[2])\n",
    "        score = 0.4 * score_views + 0.3 * score_depth + 0.3 * score_frp\n",
    "        return score, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4c7059b-ef22-4f2d-be9b-8d9a48388ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(df, target, importance=0.2):\n",
    "    top_importance_features = {}\n",
    "    for col in df:\n",
    "        params = {'learning_rate': 0.1, \n",
    "          'depth': 6, \n",
    "          'l2_leaf_reg': 3, \n",
    "          'loss_function': 'MultiRMSE', \n",
    "          'eval_metric': 'MultiRMSE', \n",
    "          'task_type': 'CPU', \n",
    "          'iterations': 400,\n",
    "          'od_type': 'Iter', \n",
    "          'boosting_type': 'Plain', \n",
    "          'bootstrap_type': 'Bernoulli', \n",
    "          'allow_const_label': True, \n",
    "          'logging_level': 'Silent',\n",
    "          'random_state': RANDOM_STATE\n",
    "         }\n",
    "        gbt = CatBoostRegressor(**params)\n",
    "        OHE = OHE_for_list(prefix=False)\n",
    "        gbt.fit(OHE.fit_transform(df[col]), target)\n",
    "        feature_importances = pd.DataFrame(gbt.feature_importances_, index=OHE.get_feature_names_out(), columns=['importance'])\n",
    "        top_importance_features[col] = feature_importances[feature_importances.importance > importance].index.to_list()\n",
    "        del gbt\n",
    "    return top_importance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ebc21ff-671a-4255-8b71-c68aa8b5a602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = get_top_features(X[['authors', 'tags']], y, 0.1)\n",
    "len(unique['authors']) + len(unique['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35b9dad6-5b41-4357-9ed0-04991106fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Do_nothing(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        self.feature_names = np.array(X.columns)\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self, *args):\n",
    "        return self.feature_names "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fbfb7-7671-48fd-8e6c-7424a8948841",
   "metadata": {},
   "source": [
    "## Обработка датасета в основном пайплайне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "607fcdb0-f44f-48b9-b365-46ab5b67fbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ПАЙПЛАЙН ОБРАБОТКИ КОЛОНОК\n",
    "impute = Pipeline(steps=[('Imputer', SimpleImputer(missing_values=0.0, strategy='median')),\n",
    "                        ('StdScaler', StandardScaler())])\n",
    "\n",
    "col_transformer = ColumnTransformer([('Do_nothing', Do_nothing(), ['num_authors', 'num_tags', 'num_words', 'USD_difference', 'is_group_authors', 'category']),\n",
    "                                     ('DatetimeEncoder', DatetimeEncoder(), 'publish_date'),\n",
    "                                     ('Imputer', SimpleImputer(missing_values=0.0, strategy='median'), ['ctr']),\n",
    "                                     ('TitleTransformer', TitleEncoderDP(distance_threshold=1.3), 'clean_title'),\n",
    "                                     ('TitleTransformerTags', TitleEncoderDP(distance_threshold=1.3), 'tags_text'),\n",
    "                                     ('OHE', OHE_for_list(unique_values=unique), ['authors', 'tags'])\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f10673e7-c272-41a5-beef-c243f2d88fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБРАБОТКА ДАТАСЕТА\n",
    "X_train = pd.DataFrame(col_transformer.fit_transform(X), columns=col_transformer.get_feature_names_out())\n",
    "X_test_final = pd.DataFrame(col_transformer.transform(X_test_final), columns=col_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cf161eba-b897-4b57-91e6-c58a736e2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАЗБИВКА ТРЕНИРОВОЧНЫХ ДАННЫХ НА ТРЕНИРОВОЧНЫЕ И ВАЛИДАЦИОННЫЕ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0adcd4-598e-4f8f-aa17-459e6f9bf90a",
   "metadata": {},
   "source": [
    "## Обучение и валидация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d39b81b-0659-4bb9-985b-e359857e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_test, pred):\n",
    "    score_views = r2_score(y_test[\"views\"], pred[:,0])\n",
    "    score_depth = r2_score(y_test[\"depth\"], pred[:,1])\n",
    "    score_frp = r2_score(y_test[\"full_reads_percent\"], pred[:,2])\n",
    "    score = 0.4 * score_views + 0.3 * score_depth + 0.3 * score_frp\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea845da0-3b79-45a5-8a68-21e0dab65021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\miniconda3\\lib\\site-packages\\catboost\\core.py:1705: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1mUntyped global name 'r2_score':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_3976\\784866544.py\", line 11:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x2c118490>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'learning_rate': 0.1, \n",
    "          'depth': 6, \n",
    "          'l2_leaf_reg': 3, \n",
    "          'loss_function': 'MultiRMSE', \n",
    "          'eval_metric': Custom_R2_Metric(), \n",
    "          'task_type': 'CPU', \n",
    "          'iterations': 800,\n",
    "          'od_type': 'Iter', \n",
    "          'boosting_type': 'Plain', \n",
    "          'bootstrap_type': 'Bernoulli', \n",
    "          'allow_const_label': True, \n",
    "          'logging_level': 'Silent',\n",
    "          'random_state': RANDOM_STATE\n",
    "         }\n",
    "cb = CatBoostRegressor(cat_features=[4,5,6,7,8,9,12], **params)\n",
    "cb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9866cf0-7e9d-4754-91e4-08402e57bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7291965606977935 0.6471294859055642\n",
      "r2 scores for views, depth and full_read_percent TRAIN: [0.7583251580245085, 0.8063808110952827, 0.6131741805313506]\n",
      "r2 scores for views, depth and full_read_percent TEST: [0.7023389767235608, 0.7501619387222755, 0.47048437866485715]\n"
     ]
    }
   ],
   "source": [
    "train_pred = cb.predict(X_train)\n",
    "test_pred = cb.predict(X_test)\n",
    "print (get_score(y_train, train_pred), get_score(y_test, test_pred))\n",
    "print('r2 scores for views, depth and full_read_percent TRAIN:', [r2_score(y_train.iloc[:,x], train_pred[:,x]) for x in range(3)])\n",
    "print('r2 scores for views, depth and full_read_percent TEST:', [r2_score(y_test.iloc[:,x], test_pred[:,x]) for x in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af98c7ab-bdc7-48ab-8941-134402351a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(cb.feature_importances_, index=X_train.drop('views', axis=1).columns)\n",
    "feature_importances.sort_values(ascending=False).to_csv('feat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0bbc7-7e46-4551-8961-5a9238487424",
   "metadata": {},
   "source": [
    "## Сохранение сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be769711-3b35-419e-9eb5-81332819d3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = cb.predict(X_test_final)\n",
    "save_solution(pred, 'solution_9.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
